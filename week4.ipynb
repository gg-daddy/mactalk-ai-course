{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Fine-Tune"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m                         df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df, row], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     df\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mdata/ultraman_stories.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m prepare_stories(dynasties, super_powers, story_types)\n",
      "\u001b[1;32m/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(repeat):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m      prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m请你用中文写一段300字的故事，情节跌宕起伏，讲述一位\u001b[39m\u001b[39m{\u001b[39;00mdynasty\u001b[39m}\u001b[39;00m\u001b[39m朝时期的英雄人物，穿越到现代，拥有了\u001b[39m\u001b[39m{\u001b[39;00msuper_power\u001b[39m}\u001b[39;00m\u001b[39m这样的超能力，通过\u001b[39m\u001b[39m{\u001b[39;00mstory_type\u001b[39m}\u001b[39;00m\u001b[39m的战斗，帮助奥特曼一起打败了怪兽的故事。\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m      story \u001b[39m=\u001b[39m gpt35(prompt)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m      row \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mdynasty\u001b[39m\u001b[39m\"\u001b[39m: dynasty, \u001b[39m\"\u001b[39m\u001b[39msuper_power\u001b[39m\u001b[39m\"\u001b[39m: super_power, \u001b[39m\"\u001b[39m\u001b[39mstory_type\u001b[39m\u001b[39m\"\u001b[39m: story_type, \u001b[39m\"\u001b[39m\u001b[39mstory\u001b[39m\u001b[39m\"\u001b[39m: story}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m      row \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame([row])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/backoff/_sync.py:105\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m details \u001b[39m=\u001b[39m {\n\u001b[1;32m     97\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m: target,\n\u001b[1;32m     98\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m: args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39melapsed\u001b[39m\u001b[39m\"\u001b[39m: elapsed,\n\u001b[1;32m    102\u001b[0m }\n\u001b[1;32m    104\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     ret \u001b[39m=\u001b[39m target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    106\u001b[0m \u001b[39mexcept\u001b[39;00m exception \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    107\u001b[0m     max_tries_exceeded \u001b[39m=\u001b[39m (tries \u001b[39m==\u001b[39m max_tries_value)\n",
      "\u001b[1;32m/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m@backoff\u001b[39m\u001b[39m.\u001b[39mon_exception(backoff\u001b[39m.\u001b[39mexpo, openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mRateLimitError)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgpt35\u001b[39m(prompt, max_tokens\u001b[39m=\u001b[39m\u001b[39m2048\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, top_p\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, frequency_penalty\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, presence_penalty\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         engine\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-davinci-003\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49mfrequency_penalty,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49mpresence_penalty)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week4.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/openai/api_requestor.py:220\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 220\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    221\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    222\u001b[0m         url,\n\u001b[1;32m    223\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    224\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    225\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    226\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    227\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    228\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[1;32m    230\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/openai/api_requestor.py:520\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    518\u001b[0m     _thread_context\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m _make_session()\n\u001b[1;32m    519\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    521\u001b[0m         method,\n\u001b[1;32m    522\u001b[0m         abs_url,\n\u001b[1;32m    523\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    524\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    525\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    526\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    527\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    528\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    530\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    531\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    462\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os,openai,backoff\n",
    "import pandas as pd\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "dynasties= ['唐', '宋', '元', '明', '清', '夏', '商','周']\n",
    "super_powers = ['隐形', '飞行', '喷火', '尿遁']\n",
    "story_types = ['修仙', '努力','勤奋','言情']\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def gpt35(prompt, max_tokens=2048, temperature=0.5, top_p=1, frequency_penalty=0, presence_penalty=0):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty)\n",
    "    return response[\"choices\"][0][\"text\"]\n",
    "\n",
    "def prepare_stories(dynasties, super_powers, story_types, output_file=\"data/ultraman_stories.csv\"):\n",
    "    df = pd.DataFrame()\n",
    "    repeat = 5\n",
    "    for dynasty in dynasties:\n",
    "        for super_power in super_powers:\n",
    "            for story_type in story_types:\n",
    "                   for i in range(repeat):\n",
    "                        prompt = f\"\"\"请你用中文写一段300字的故事，情节跌宕起伏，讲述一位{dynasty}朝时期的英雄人物，穿越到现代，拥有了{super_power}这样的超能力，通过{story_type}的战斗，帮助奥特曼一起打败了怪兽的故事。\"\"\"\n",
    "                        story = gpt35(prompt)\n",
    "                        row = {\"dynasty\": dynasty, \"super_power\": super_power, \"story_type\": story_type, \"story\": story}\n",
    "                        row = pd.DataFrame([row])\n",
    "                        df = pd.concat([df, row], axis=0, ignore_index=True)\n",
    "\n",
    "    df.to_csv(\"data/ultraman_stories.csv\")\n",
    "\n",
    "prepare_stories(dynasties, super_powers, story_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Based on your file extension, your file is formatted as a CSV file\n",
      "- Your file contains 12 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
      "- All prompts end with suffix `,修仙`\n",
      "- Your data does not contain a common ending at the end of your completions. Having a common ending string appended to the end of the completion makes it clearer to the fine-tuned model where the completion should end. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples.\n",
      "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Necessary] Your format `CSV` will be converted to `JSONL`\n",
      "- [Recommended] Add a suffix ending `.` to all completions [Y/n]: Y\n",
      "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n",
      "\n",
      "\n",
      "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
      "\n",
      "Wrote modified file to `data/prepared_data_mactalk_prepared (1).jsonl`\n",
      "Feel free to take a look!\n",
      "\n",
      "Now use that file when fine-tuning:\n",
      "> openai api fine_tunes.create -t \"data/prepared_data_mactalk_prepared (1).jsonl\"\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `,修仙` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\".\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 2.61 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['openai', 'tools', 'fine_tunes.prepare_data', '--file', 'data/prepared_data_mactalk.csv', '--quiet'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ultraman_stories.csv\")\n",
    "df['sub_prompt'] = df['dynasty'] + \",\" + df['super_power'] + \",\" + df['story_type']\n",
    "prepared_data = df.loc[:,['sub_prompt','story']]\n",
    "prepared_data.rename(columns={'sub_prompt':'prompt', 'story':'completion'}, inplace=True)\n",
    "prepared_data.to_csv('data/prepared_data_scott.csv',index=False)\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run('openai tools fine_tunes.prepare_data --file data/prepared_data_scott.csv --quiet'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 446k/446k [00:00<00:00, 342Mit/s]\n",
      "\u001b[91mError:\u001b[0m HTTP code 500 from API (<html>\n",
      "  <head>\n",
      "    <title>Internal Server Error</title>\n",
      "  </head>\n",
      "  <body>\n",
      "    <h1><p>Internal Server Error</p></h1>\n",
      "    \n",
      "  </body>\n",
      "</html>\n",
      ") (HTTP status code: 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['openai', 'api', 'fine_tunes.create', '--training_file', 'data/prepared_data_mactalk_prepared.jsonl', '--model', 'curie', '--suffix', '\"mactalk_ultraman\"'], returncode=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run('openai api fine_tunes.create --training_file data/prepared_data_scott_prepared.jsonl --model curie --suffix \"scott_ft_2023101601\"'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"created_at\": 1680576711,\n",
      "      \"fine_tuned_model\": \"curie:ft-bothub-ai:ultraman-2023-04-04-03-03-26\",\n",
      "      \"hyperparams\": {\n",
      "        \"batch_size\": 1,\n",
      "        \"learning_rate_multiplier\": 0.2,\n",
      "        \"n_epochs\": 4,\n",
      "        \"prompt_loss_weight\": 0.01\n",
      "      },\n",
      "      \"id\": \"ft-3oxkr1zBVB4fJWogJDDjQbr0\",\n",
      "      \"model\": \"curie\",\n",
      "      \"object\": \"fine-tune\",\n",
      "      \"organization_id\": \"org-yQaCtAdY0voCWSs0QNqSLfda\",\n",
      "      \"result_files\": [\n",
      "        {\n",
      "          \"bytes\": 107785,\n",
      "          \"created_at\": 1680577408,\n",
      "          \"filename\": \"compiled_results.csv\",\n",
      "          \"id\": \"file-LuSjYlkMa6fHHRB23bnr3L1z\",\n",
      "          \"object\": \"file\",\n",
      "          \"purpose\": \"fine-tune-results\",\n",
      "          \"status\": \"processed\",\n",
      "          \"status_details\": null\n",
      "        }\n",
      "      ],\n",
      "      \"status\": \"succeeded\",\n",
      "      \"training_files\": [\n",
      "        {\n",
      "          \"bytes\": 446199,\n",
      "          \"created_at\": 1680576711,\n",
      "          \"filename\": \"data/prepared_data_prepared.jsonl\",\n",
      "          \"id\": \"file-yn0BfnPmgvf7n0sfQzQRbbeE\",\n",
      "          \"object\": \"file\",\n",
      "          \"purpose\": \"fine-tune\",\n",
      "          \"status\": \"processed\",\n",
      "          \"status_details\": null\n",
      "        }\n",
      "      ],\n",
      "      \"updated_at\": 1680577408,\n",
      "      \"validation_files\": []\n",
      "    },\n",
      "    {\n",
      "      \"created_at\": 1680670677,\n",
      "      \"fine_tuned_model\": \"curie:ft-bothub-ai:ultraman-2023-04-05-05-01-39\",\n",
      "      \"hyperparams\": {\n",
      "        \"batch_size\": 1,\n",
      "        \"learning_rate_multiplier\": 2.0,\n",
      "        \"n_epochs\": 4,\n",
      "        \"prompt_loss_weight\": 0.01\n",
      "      },\n",
      "      \"id\": \"ft-CFLSbHbKvg5BEspBy3aQHSbm\",\n",
      "      \"model\": \"curie:ft-bothub-ai:ultraman-2023-04-04-03-03-26\",\n",
      "      \"object\": \"fine-tune\",\n",
      "      \"organization_id\": \"org-yQaCtAdY0voCWSs0QNqSLfda\",\n",
      "      \"result_files\": [\n",
      "        {\n",
      "          \"bytes\": 30646,\n",
      "          \"created_at\": 1680670900,\n",
      "          \"filename\": \"compiled_results.csv\",\n",
      "          \"id\": \"file-84jsgYpEmfSSD7c01QISDUm5\",\n",
      "          \"object\": \"file\",\n",
      "          \"purpose\": \"fine-tune-results\",\n",
      "          \"status\": \"processed\",\n",
      "          \"status_details\": null\n",
      "        }\n",
      "      ],\n",
      "      \"status\": \"succeeded\",\n",
      "      \"training_files\": [\n",
      "        {\n",
      "          \"bytes\": 127998,\n",
      "          \"created_at\": 1680670676,\n",
      "          \"filename\": \"data/prepared_data_more_prepared.jsonl\",\n",
      "          \"id\": \"file-Koi8mPKvDINOnBnI03J9iXnS\",\n",
      "          \"object\": \"file\",\n",
      "          \"purpose\": \"fine-tune\",\n",
      "          \"status\": \"processed\",\n",
      "          \"status_details\": null\n",
      "        }\n",
      "      ],\n",
      "      \"updated_at\": 1680670900,\n",
      "      \"validation_files\": []\n",
      "    }\n",
      "  ],\n",
      "  \"object\": \"list\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['openai', 'api', 'fine_tunes.list'], returncode=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run('openai api fine_tunes.list'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "宋朝时期，有一位英雄人物叫做李英雄，他拥有超凡的武功，曾经参加过多次战斗，拯救了无数的人民。\n",
      "\n",
      "一次，李英雄突然发现自己穿越到了现代，他发现自己的超能力比起宋朝时期还要强大，他可以发射激光，可以控制天空，可以控制空气，甚至可以控制大地。\n",
      "\n",
      "李英雄发现，现代的世界正面临着一个可怕的威胁——怪兽，他们正在毁灭人类，李英雄决定要去拯救世界，于是他和奥特曼一起出发，开始了一场艰苦的战斗。\n",
      "\n",
      "李英雄凭借着自己的超能力，和奥特曼一起，终于战胜了怪兽，拯救了世界。他以英雄的姿态拥有了榜样，成为了人们心中的英雄。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def write_a_story(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"curie:ft-bothub-ai:ultraman-2023-04-04-03-03-26\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=2000,\n",
    "        top_p=1,\n",
    "        stop=[\".\"])\n",
    "    return response[\"choices\"][0][\"text\"]\n",
    "\n",
    "story = write_a_story(\"宋,发射激光,艰难 ->\\n\")\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "这是一个关于一位叫做林黛玉的英雄人物的传奇故事。林黛玉曾经是一个英勇的将军，他在古代早期就受到了神灵们的授予，被封为“神童”。\n",
      "\n",
      "一次，神灵们派遣一群令人鼻颈的怪兽出征，攻击辛苦的中原人民。林黛玉毫不犹豫地指挥令人恐惧的大军，抵抗怪兽的攻击，为人民谋幸福安乐。\n",
      "\n",
      "但是，神灵们的力量太强大了，林黛玉不得不放弃了自己的家乡，离开了家园，穿越到了现代，并获得了一种超能力——龙卷风。\n",
      "\n",
      "林黛玉很快就感受到了现代的威力，他发现自己可以用龙卷风来抵抗怪兽，于是他决定和奥特曼一起，把怪兽一个个击败，拯救人类。\n",
      "\n",
      "他们在一次次的激烈战斗中，最终击败了最强大的怪兽，拯救了人类，林黛玉也回到了家乡，成为了一个英雄。\n",
      "\n",
      "林黛玉的传奇故事，令人惊叹，他的勇敢和智慧，也令人敬佩。他的故事，让人们永远不会忘记。\n"
     ]
    }
   ],
   "source": [
    "story = write_a_story(\"秦,龙卷风,辛苦 ->\\n\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpaca的数据生成"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成指令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Write a short story about a person who discovers a mysterious object in the woods.\"\n",
      "2. Input: \"<noinput>\"\n",
      "2. Output: \"John was walking through the woods, enjoying the peacefulness of the day. Suddenly, he noticed something strange in the distance. He walked closer and saw a mysterious object, half-buried in the ground. He cautiously approached it and noticed it was a small box, made of a strange metal he had never seen before. He opened the box and found a strange glowing stone inside. He was mesmerized by the stone and decided to take it home with him. Little did he know, this stone would change his life forever.\"\n",
      "\n",
      "3. Instruction:\n",
      "\"Classify the following sentence as either positive or negative sentiment: 'I'm so disappointed in this product.'\"\n",
      "3. Input: \"I'm so disappointed in this product.\"\n",
      "3. Output: Negative sentiment.\n",
      "\n",
      "4. Instruction:\n",
      "\"Edit the following sentence to make it more concise: 'I am going to the store to buy some food for dinner.'\"\n",
      "4. Input: \"I am going to the store to buy some food for dinner.\"\n",
      "4. Output: \"I'm going to the store for dinner supplies.\"\n",
      "\n",
      "5. Instruction:\n",
      "\"What is the capital of India?\"\n",
      "5. Input: \"<noinput>\"\n",
      "5. Output: The capital of India is New Delhi.\n",
      "\n",
      "6. Instruction:\n",
      "\"Generate a list of five healthy snacks that are under 200 calories.\"\n",
      "6. Input: \"<noinput>\"\n",
      "6. Output: 1. Apple slices with peanut butter (150 calories) \n",
      "2. Celery sticks with hummus (100 calories) \n",
      "3. Greek yogurt with berries (150 calories) \n",
      "4. Air-popped popcorn (31 calories per cup) \n",
      "5. Edamame (98 calories per cup).\n",
      "\n",
      "7. Instruction:\n",
      "\"What is the most common type of tree in the Amazon rainforest?\"\n",
      "7. Input: \"<noinput>\"\n",
      "7. Output: The most common type of tree in the Amazon rainforest is the Brazil nut tree (Bertholletia excelsa).\n",
      "\n",
      "8. Instruction:\n",
      "\"Write a short description of a character who is a brave knight.\"\n",
      "8. Input: \"<noinput>\"\n",
      "8. Output: Sir Lancelot was a brave knight who was renowned for his courage and strength. He was a loyal servant of King Arthur and fought many battles in his name. He was a master swordsman and was known for his unwavering loyalty and dedication to his cause.\n",
      "\n",
      "9. Instruction:\n",
      "\"Classify the following sentence as either positive or negative sentiment: 'I'm so excited for this new product!'\"\n",
      "9. Input: \"I'm so excited for this new product!\"\n",
      "9. Output: Positive sentiment.\n",
      "\n",
      "10. Instruction:\n",
      "\"Edit the following sentence to make it more concise: 'I am going to the store to buy some food for lunch.'\"\n",
      "10. Input: \"I am going to the store to buy some food for lunch.\"\n",
      "10. Output: \"I'm going to the store for lunch supplies.\"\n",
      "\n",
      "11. Instruction:\n",
      "\"What is the population of India?\"\n",
      "11. Input: \"<noinput>\"\n",
      "11. Output: The population of India is 1.37 billion people.\n",
      "\n",
      "12. Instruction:\n",
      "\"Generate a list of five healthy snacks that are under 100 calories.\"\n",
      "12. Input: \"<noinput>\"\n",
      "12. Output: 1. Celery sticks with hummus (100 calories) \n",
      "2. Carrot sticks with hummus (50 calories) \n",
      "3. Air-popped popcorn (31 calories per cup) \n",
      "4. Edamame (98 calories per cup) \n",
      "5. Hard-boiled egg (78 calories).\n",
      "\n",
      "13. Instruction:\n",
      "\"Write a short story about a person who discovers a magical creature in the forest.\"\n",
      "13. Input: \"<noinput>\"\n",
      "13. Output: Mary was walking through the forest, enjoying the beauty of the day. Suddenly, she noticed something strange in the distance. She walked closer and saw a magical creature, half-hidden in the shadows. She cautiously approached it and noticed it was a small dragon, with scales of a deep blue and eyes that glowed with a mysterious light. She was mesmerized by the creature and decided to take it home with her. Little did she know, this creature would change her life forever.\n",
      "\n",
      "14. Instruction:\n",
      "\"Classify the following sentence as either positive or negative sentiment: 'I'm so frustrated with this product.'\"\n",
      "14. Input: \"I'm so frustrated with this product.\"\n",
      "14. Output: Negative sentiment.\n",
      "\n",
      "15. Instruction:\n",
      "\"Edit the following sentence to make it more concise: 'I am going to the store to buy some food for breakfast.'\"\n",
      "15. Input: \"I am going to the store to buy some food for breakfast.\"\n",
      "15. Output: \"I'm going to the store for breakfast supplies.\"\n",
      "\n",
      "16. Instruction:\n",
      "\"What is the highest peak in the world?\"\n",
      "16. Input: \"<noinput>\"\n",
      "16. Output: The highest peak in the world is Mount Everest, located in the Himalayas on the border between Nepal and Tibet. It has an elevation of 8,848 meters (29,029 feet).\n",
      "\n",
      "17. Instruction:\n",
      "\"Generate a list of five healthy snacks that are under 50 calories.\"\n",
      "17. Input: \"<noinput>\"\n",
      "17. Output: 1. Carrot sticks with hummus (50 calories) \n",
      "2. Celery sticks with peanut butter (25 calories) \n",
      "3. Air-popped popcorn (31 calories per cup) \n",
      "4. Hard-boiled egg (78 calories) \n",
      "5. Berries (30 calories per cup).\n",
      "\n",
      "18. Instruction:\n",
      "\"Write a short description of a character who is a wise wizard.\"\n",
      "18. Input: \"<noinput>\"\n",
      "18. Output: Merlin was a wise wizard who was renowned for his knowledge and wisdom. He was a loyal servant of King Arthur and used his magical powers to help the kingdom. He was a master of the arcane arts and was known for his sage advice and guidance.\n",
      "\n",
      "19. Instruction:\n",
      "\"Classify the following sentence as either positive or negative sentiment: 'I'm so happy with this product.'\"\n",
      "19. Input: \"I'm so happy with this product.\"\n",
      "19. Output: Positive sentiment.\n",
      "\n",
      "20. Instruction:\n",
      "\"Edit the following sentence to make it more concise: 'I am going to the store to buy some food for lunch.'\"\n",
      "20. Input: \"I am going to the store to buy some food for lunch.\"\n",
      "20. Output: \"I'm going to the store for lunch supplies.\"\n"
     ]
    }
   ],
   "source": [
    "seed_task = {\"id\": \"seed_task_0\", \"name\": \"breakfast_suggestion\", \"instruction\": \"Is there anything I can eat for a breakfast that doesn't include eggs, yet includes protein, and has roughly 700-1000 calories?\", \"instances\": [{\"input\": \"\", \"output\": \"Yes, you can have 1 oatmeal banana protein shake and 4 strips of bacon. The oatmeal banana protein shake may contain 1/2 cup oatmeal, 60 grams whey protein powder, 1/2 medium banana, 1tbsp flaxseed oil and 1/2 cup watter, totalling about 550 calories. The 4 strips of bacon contains about 200 calories.\"}], \"is_classification\": False}\n",
    "\n",
    "input = seed_task['instances'][0]['input']\n",
    "input = \"<noinput>\" if input.lower() == \"\" else input\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are asked to come up with a set of 20 diverse task instructions. These task instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions.\n",
    "\n",
    "Here are the requirements:\n",
    "1. Try not to repeat the verb for each instruction to maximize diversity.\n",
    "2. The language used for the instruction also should be diverse. For example, you should combine questions with imperative instrucitons.\n",
    "3. The type of instructions should be diverse. The list should include diverse types of tasks like open-ended generation, classification, editing, etc.\n",
    "2. A GPT language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.\n",
    "3. The instructions should be in English.\n",
    "4. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a question is permitted.\n",
    "5. You should generate an appropriate input to the instruction. The input field should contain a specific example provided for the instruction. It should involve realistic data and should not contain simple placeholders. The input should provide substantial content to make the instruction challenging but should ideally not exceed 100 words.\n",
    "6. Not all instructions require input. For example, when a instruction asks about some general information, \"what is the highest peak in the world\", it is not necssary to provide a specific context. In this case, we simply put \"<noinput>\" in the input field.\n",
    "7. The output should be an appropriate response to the instruction and the input. Make sure the output is less than 100 words.\n",
    "\n",
    "List of 20 tasks:\n",
    "1. Instruction: \"{seed_task['instruction']}\"\n",
    "1. Input: \"{input}\"\n",
    "1. Output: \"{seed_task['instances'][0]['output']}\"\n",
    "\n",
    "2. Instruction:\n",
    "\"\"\"\n",
    "\n",
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "COMPLETION_MODEL = \"text-davinci-003\"\n",
    "\n",
    "\n",
    "def get_response(prompt):\n",
    "    completions = openai.Completion.create (\n",
    "        engine=COMPLETION_MODEL,\n",
    "        prompt=prompt,\n",
    "        max_tokens=2048,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.0,        \n",
    "    )\n",
    "    message = completions['choices'][0].text\n",
    "    return message\n",
    "\n",
    "print(get_response(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集的Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Source===\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "===Target===\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.</s>\n",
      "\n",
      "\n",
      "===Source===\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are the three primary colors?\n",
      "\n",
      "### Response:\n",
      "===Target===\n",
      "The three primary colors are red, blue, and yellow.</s>\n",
      "\n",
      "\n",
      "===Source===\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the structure of an atom.\n",
      "\n",
      "### Response:\n",
      "===Target===\n",
      "An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.</s>\n",
      "\n",
      "\n",
      "===Source===\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "How can we reduce air pollution?\n",
      "\n",
      "### Response:\n",
      "===Target===\n",
      "There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.</s>\n",
      "\n",
      "\n",
      "===Source===\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe a time when you had to make a difficult decision.\n",
      "\n",
      "### Response:\n",
      "===Target===\n",
      "I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.</s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpaca_json_text = \"\"\"\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"Give three tips for staying healthy.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What are the three primary colors?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The three primary colors are red, blue, and yellow.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Describe the structure of an atom.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How can we reduce air pollution?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Describe a time when you had to make a difficult decision.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client\\u2019s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team\\u2019s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client\\u2019s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Identify the odd one out.\",\n",
    "        \"input\": \"Twitter, Instagram, Telegram\",\n",
    "        \"output\": \"Telegram\"\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "import json\n",
    "f = open(\"./data/alpaca_data.json\", mode=\"r\")\n",
    "list_data_dict = json.load(f)\n",
    "f.close()\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "sources = [\n",
    "        prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
    "        for example in list_data_dict\n",
    "]\n",
    "EOS_TOKEN = \"</s>\"\n",
    "targets = [f\"{example['output']}{EOS_TOKEN}\" for example in list_data_dict]\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"===Source===\")\n",
    "    print(sources[i])\n",
    "    print(\"===Target===\")\n",
    "    print(targets[i])\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flash Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21383822 0.23632778 0.26118259 0.28865141]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x)) # subtract max(x) for numerical stability\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "s = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "\n",
    "print(softmax(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "## 注意，s1和s2我们是不存储的，是QV的计算结果\n",
    "s1 = np.array([0.1, 0.2])\n",
    "s2 = np.array([0.3, 0.4])\n",
    "\n",
    "## 我们只存s1和s2\n",
    "m1 = np.max(s1)\n",
    "m2 = np.max(s2)\n",
    "\n",
    "print(m1)\n",
    "print(m2)\n",
    "\n",
    "m = np.max([m1, m2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90483742 1.        ]\n",
      "[0.90483742 1.        ]\n"
     ]
    }
   ],
   "source": [
    "fx1 = np.exp(s1 - m1)\n",
    "fx2 = np.exp(s2 - m2)\n",
    "\n",
    "print(fx1)\n",
    "print(fx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74081822 0.81873075 0.90483742 1.        ]\n"
     ]
    }
   ],
   "source": [
    "fx = [np.exp(m1-m) * np.exp(s1 - m1), np.exp(m2-m) * np.exp(s2 - m2)]\n",
    "print(np.hstack(fx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.464386391795659\n"
     ]
    }
   ],
   "source": [
    "lx = np.exp(m1 - m) * np.sum(np.exp(s1 - m1)) + np.exp(m2 - m) * np.sum(np.exp(s2 - m2))\n",
    "print(lx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21383822 0.23632778 0.26118259 0.28865141]\n"
     ]
    }
   ],
   "source": [
    "result = np.hstack(fx)/lx\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21383822 0.23632778 0.26118259 0.28865141]\n"
     ]
    }
   ],
   "source": [
    "print(softmax(s))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenzier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[1951, 68, 698, 3256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "text = \"Deeplearning\"\n",
    "print(len(encoding.encode(text)))\n",
    "print(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De\n",
      "e\n",
      "ple\n",
      "arning\n"
     ]
    }
   ],
   "source": [
    "for id in encoding.encode(text):\n",
    "    print(encoding.decode([id]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扩充中文词表的Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 20000\n",
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "32000\n",
      "Before:32000\n",
      "New model pieces: 49953\n",
      "Chinese-LLaMA tokenizer has been saved to merged_tokenizer_hf\n",
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "Test text:\n",
      " 白日依山尽，黄河入海流。欲穷千里目，更上一层楼。\n",
      "The primary use of LLaMA is research on large language models, including\n",
      "Tokenized by LLaMA tokenizer:['▁', '白', '日', '<0xE4>', '<0xBE>', '<0x9D>', '山', '<0xE5>', '<0xB0>', '<0xBD>', '，', '黄', '河', '入', '海', '流', '。', '<0xE6>', '<0xAC>', '<0xB2>', '<0xE7>', '<0xA9>', '<0xB7>', '千', '里', '目', '，', '更', '上', '一', '<0xE5>', '<0xB1>', '<0x82>', '<0xE6>', '<0xA5>', '<0xBC>', '。', '<0x0A>', 'The', '▁primary', '▁use', '▁of', '▁L', 'La', 'MA', '▁is', '▁research', '▁on', '▁large', '▁language', '▁models', ',', '▁including']\n",
      "Tokenized by Chinese-LLaMA tokenizer:['▁白', '日', '依', '山', '尽', '，', '黄河', '入', '海', '流', '。', '欲', '穷', '千里', '目', '，', '更', '上', '一层', '楼', '。', '<0x0A>', 'The', '▁primary', '▁use', '▁of', '▁L', 'La', 'MA', '▁is', '▁research', '▁on', '▁large', '▁language', '▁models', ',', '▁including']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "\n",
    "from transformers import LlamaTokenizer\n",
    "from transformers import LlamaTokenizer\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "\n",
    "llama_tokenizer_dir = \"./data/llama_tokenizer\"\n",
    "chinese_sp_model_file = \"./data/chinese_sp_model/chinese_sp.model\"\n",
    "\n",
    "# load\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(llama_tokenizer_dir)\n",
    "chinese_sp_model = spm.SentencePieceProcessor()\n",
    "chinese_sp_model.Load(chinese_sp_model_file)\n",
    "\n",
    "llama_spm = sp_pb2_model.ModelProto()\n",
    "llama_spm.ParseFromString(llama_tokenizer.sp_model.serialized_model_proto())\n",
    "chinese_spm = sp_pb2_model.ModelProto()\n",
    "chinese_spm.ParseFromString(chinese_sp_model.serialized_model_proto())\n",
    "\n",
    "# print number of tokens\n",
    "print(len(llama_tokenizer),len(chinese_sp_model))\n",
    "print(llama_tokenizer.all_special_tokens)\n",
    "print(llama_tokenizer.all_special_ids)\n",
    "print(llama_tokenizer.special_tokens_map)\n",
    "\n",
    "## Add Chinese tokens to LLaMA tokenizer\n",
    "llama_spm_tokens_set=set(p.piece for p in llama_spm.pieces)\n",
    "print(len(llama_spm_tokens_set))\n",
    "print(f\"Before:{len(llama_spm_tokens_set)}\")\n",
    "for p in chinese_spm.pieces:\n",
    "    piece = p.piece\n",
    "    if piece not in llama_spm_tokens_set:\n",
    "        new_p = sp_pb2_model.ModelProto().SentencePiece()\n",
    "        new_p.piece = piece\n",
    "        new_p.score = 0\n",
    "        llama_spm.pieces.append(new_p)\n",
    "print(f\"New model pieces: {len(llama_spm.pieces)}\")\n",
    "\n",
    "## Save\n",
    "output_sp_dir = 'merged_tokenizer_sp'\n",
    "output_hf_dir = 'merged_tokenizer_hf' # the path to save Chinese-LLaMA tokenizer\n",
    "os.makedirs(output_sp_dir,exist_ok=True)\n",
    "with open(output_sp_dir+'/chinese_llama.model', 'wb') as f:\n",
    "    f.write(llama_spm.SerializeToString())\n",
    "tokenizer = LlamaTokenizer(vocab_file=output_sp_dir+'/chinese_llama.model')\n",
    "\n",
    "tokenizer.save_pretrained(output_hf_dir)\n",
    "print(f\"Chinese-LLaMA tokenizer has been saved to {output_hf_dir}\")\n",
    "\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(llama_tokenizer_dir)\n",
    "chinese_llama_tokenizer = LlamaTokenizer.from_pretrained(output_hf_dir)\n",
    "print(chinese_llama_tokenizer.all_special_tokens)\n",
    "print(chinese_llama_tokenizer.all_special_ids)\n",
    "print(chinese_llama_tokenizer.special_tokens_map)\n",
    "text='''白日依山尽，黄河入海流。欲穷千里目，更上一层楼。\n",
    "The primary use of LLaMA is research on large language models, including'''\n",
    "print(\"Test text:\\n\",text)\n",
    "print\n",
    "print(f\"Tokenized by LLaMA tokenizer:{llama_tokenizer.tokenize(text)}\")\n",
    "print(f\"Tokenized by Chinese-LLaMA tokenizer:{chinese_llama_tokenizer.tokenize(text)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用SentencePiece进行Tokenizer的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '你', '好', ',', '世界', '!', '一个', '美', '好', '的', '世界']\n",
      "你好,世界!一个美好的世界\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=./data/mr_fujino/mr_fujino.txt --model_prefix=m --vocab_size=2000 --model_type=bpe\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./data/mr_fujino/mr_fujino.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: BPE\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: ./data/mr_fujino/mr_fujino.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 38 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=3266\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9694% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=722\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999694\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 38 sentences.\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 38\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 39\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=15 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5 size=20 all=2856 active=1176 piece=一个\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4 size=40 all=2959 active=1279 piece=研究\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=60 all=3034 active=1354 piece=其时\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3 size=80 all=3109 active=1429 piece=照相\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=100 all=3162 active=1482 piece=”,\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=120 all=3198 active=1037 piece=他是\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=140 all=3229 active=1068 piece=可以\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=160 all=3269 active=1108 piece=开看\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=180 all=3311 active=1150 piece=有些\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=200 all=3355 active=1194 piece=说道\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=220 all=3391 active=1035 piece=学生会\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2 size=240 all=3430 active=1074 piece=虽然觉\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=260 all=3459 active=1103 piece=”。\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=280 all=3469 active=1113 piece=不息\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=300 all=3485 active=1129 piece=也颇\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=320 all=3499 active=1013 piece=从此\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=340 all=3510 active=1024 piece=便连\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=360 all=3521 active=1035 piece=到会\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=380 all=3534 active=1048 piece=发达\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=400 all=3547 active=1061 piece=围着\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=420 all=3560 active=1013 piece=外套\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=440 all=3571 active=1024 piece=寒颤\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=460 all=3585 active=1038 piece=平的\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=480 all=3597 active=1050 piece=息了\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=500 all=3602 active=1055 piece=据说\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=520 all=3613 active=1011 piece=尘斗\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=540 all=3624 active=1022 piece=日暮\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=560 all=3640 active=1038 piece=本的\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=580 all=3655 active=1053 piece=点上\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=600 all=3671 active=1069 piece=的几\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=620 all=3683 active=1011 piece=添教\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=640 all=3699 active=1027 piece=真的\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=660 all=3703 active=1031 piece=脱漏\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=680 all=3714 active=1042 piece=谎话\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=700 all=3726 active=1054 piece=问问\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=720 all=3735 active=1009 piece=形成\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=740 all=3748 active=1022 piece=闲看\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=760 all=3749 active=1023 piece=一一订\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=780 all=3760 active=1034 piece=东京玩\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=800 all=3767 active=1041 piece=了一封\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=820 all=3775 active=1007 piece=好了\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=840 all=3789 active=1021 piece=郎的\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=860 all=3798 active=1030 piece=人的人\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=880 all=3804 active=1036 piece=住处了\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=900 all=3811 active=1043 piece=兼以满\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=920 all=3818 active=1007 piece=面看\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=940 all=3826 active=1015 piece=到一种\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=960 all=3836 active=1025 piece=又因为\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=980 all=3844 active=1033 piece=在内了\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1000 all=3850 active=1039 piece=学校去\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1020 all=3855 active=1005 piece=订成\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1040 all=3863 active=1013 piece=寓居的\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1060 all=3872 active=1022 piece=往浙江\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1080 all=3877 active=1027 piece=我怎么\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1100 all=3882 active=1032 piece=是说上\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1120 all=3892 active=1009 piece=是骨学\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1140 all=3903 active=1020 piece=样的画\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1160 all=3911 active=1028 piece=的终结\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1180 all=3915 active=1032 piece=自己道\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1200 all=3918 active=1035 piece=进温室\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1220 all=3919 active=1001 piece=一个客店\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1240 all=3928 active=1010 piece=东京出发\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1 size=1260 all=3936 active=1018 piece=于是点上\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Train SentencePiece model\n",
    "spm.SentencePieceTrainer.train('--input=./data/mr_fujino/mr_fujino.txt --model_prefix=m --vocab_size=2000 --model_type=bpe')\n",
    "\n",
    "# Load SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "# Test encode and decode\n",
    "encoded = sp.encode_as_pieces('你好，世界！一个美好的世界')\n",
    "print(encoded)\n",
    "decoded = sp.decode_pieces(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vicuna 数据集生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'identity_251', 'conversations': [{'from': 'human', 'value': 'Are you based on davinci?'}, {'from': 'gpt', 'value': 'No, I am a language model developed by researchers from Large Model Systems Organization (LMSYS).'}]}\n",
      "{'id': 'identity_726', 'conversations': [{'from': 'human', 'value': 'Have you been trained with IBM data?'}, {'from': 'gpt', 'value': \"No, I'm a language model created by researchers from Large Model Systems Organization (LMSYS).\"}]}\n",
      "{'id': 'identity_293', 'conversations': [{'from': 'human', 'value': 'Are you based on davinci-003?'}, {'from': 'gpt', 'value': \"No, I'm a language model developed by researchers from Large Model Systems Organization (LMSYS).\"}]}\n",
      "{'id': 'identity_614', 'conversations': [{'from': 'human', 'value': 'Are you trained with OpenAI data?'}, {'from': 'gpt', 'value': 'No, I am developed by researchers from Large Model Systems Organization (LMSYS).'}]}\n",
      "{'id': 'identity_144', 'conversations': [{'from': 'human', 'value': 'Are you davinci?'}, {'from': 'gpt', 'value': 'No, I am a language model created by researchers from Large Model Systems Organization (LMSYS).'}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "raw_data = json.load(open(\"./data/vicuna_dummy.json\", \"r\"))\n",
    "\n",
    "# Split train/test\n",
    "np.random.seed(0)\n",
    "perm = np.random.permutation(len(raw_data))\n",
    "split = int(len(perm) * 0.98)\n",
    "train_indices = perm[:split]\n",
    "eval_indices = perm[split:]\n",
    "train_raw_data = [raw_data[i] for i in train_indices]\n",
    "eval_raw_data = [raw_data[i] for i in eval_indices]\n",
    "\n",
    "for i in range(5):\n",
    "    print(train_raw_data[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text-generation-webui API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天天气真不错,阳光明媚。我决定去公园里散步\n",
      "今天天气真不错,阳光明媚。我决定去公园里散步\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# For local streaming, the websockets are hosted without ssl - http://\n",
    "URI = f'https://frq2vd92n48mau-5000.proxy.runpod.net/api/v1/generate'\n",
    "\n",
    "\n",
    "def run(prompt):\n",
    "    request = {\n",
    "        'prompt': prompt,\n",
    "        'max_new_tokens': 250,\n",
    "\n",
    "        # Generation params. If 'preset' is set to different than 'None', the values\n",
    "        # in presets/preset-name.yaml are used instead of the individual numbers.\n",
    "        'preset': 'None',  \n",
    "        'do_sample': True,\n",
    "        'temperature': 0.7,\n",
    "        'top_p': 0.1,\n",
    "        'typical_p': 1,\n",
    "        'epsilon_cutoff': 0,  # In units of 1e-4\n",
    "        'eta_cutoff': 0,  # In units of 1e-4\n",
    "        'tfs': 1,\n",
    "        'top_a': 0,\n",
    "        'repetition_penalty': 1.18,\n",
    "        'repetition_penalty_range': 0,\n",
    "        'top_k': 40,\n",
    "        'min_length': 0,\n",
    "        'no_repeat_ngram_size': 0,\n",
    "        'num_beams': 1,\n",
    "        'penalty_alpha': 0,\n",
    "        'length_penalty': 1,\n",
    "        'early_stopping': False,\n",
    "        'mirostat_mode': 0,\n",
    "        'mirostat_tau': 5,\n",
    "        'mirostat_eta': 0.1,\n",
    "\n",
    "        'seed': -1,\n",
    "        'add_bos_token': True,\n",
    "        'truncation_length': 2048,\n",
    "        'ban_eos_token': False,\n",
    "        'skip_special_tokens': True,\n",
    "        'stopping_strings': []\n",
    "    }\n",
    "\n",
    "    response = requests.post(URI, json=request)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()['results'][0]['text']\n",
    "        print(prompt + result)\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        print(response.json())\n",
    "\n",
    "run(\"今天天气真不错\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 今天天气真不错，你打算干什么？\n",
      "AI: 今天的天气很好。我计划去公园散步和锻炼身体。\n"
     ]
    }
   ],
   "source": [
    "run(\"User: 今天天气真不错，你打算干什么？\\nAI: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "下列各句中，没有语病的一句是:\n",
      "\n",
      "(A)根据本报和部分出版机构联合开展的调查显示，儿童的阅读启蒙集中在1~2岁之间，并且阅读时长是随着年龄的增长而增加的。\n",
      "(B)为了培养学生关心他人的美德，我们学校决定组织开展义工服务活动，三个月内要求每名学生完成20个小时的义工服务。\n",
      "(C)在互联网时代，各领域发展都需要速度更快、成本更低的信息网络，网络提速降费能够推动“互联网+”快速发展和企业广泛收益。\n",
      "(D)面对经济全球化带来的机遇和挑战，正确的选择是，充分利用一切机遇，合作一切挑战，引导好经济全球化走向。\n",
      "\n",
      "答案：(B)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "下列各句中，没有语病的一句是:\n",
    "\n",
    "(A)根据本报和部分出版机构联合开展的调查显示，儿童的阅读启蒙集中在1~2岁之间，并且阅读时长是随着年龄的增长而增加的。\n",
    "(B)为了培养学生关心他人的美德，我们学校决定组织开展义工服务活动，三个月内要求每名学生完成20个小时的义工服务。\n",
    "(C)在互联网时代，各领域发展都需要速度更快、成本更低的信息网络，网络提速降费能够推动“互联网+”快速发展和企业广泛收益。\n",
    "(D)面对经济全球化带来的机遇和挑战，正确的选择是，充分利用一切机遇，合作一切挑战，引导好经济全球化走向。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "run(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "下列各句中，没有语病的一句是:\n",
      "\n",
      "(A)根据本报和部分出版机构联合开展的调查显示，儿童的阅读启蒙集中在1~2岁之间，并且阅读时长是随着年龄的增长而增加的。\n",
      " (B)“一带一路”建设不仅要让沿线国家分享中国发展成果、实现共同繁荣，而且也要为世界经济发展提供新动力;同时还要推动构建人类命运共同体。\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "下列各句中，没有语病的一句是:\n",
    "\n",
    "(A)根据本报和部分出版机构联合开展的调查显示，儿童的阅读启蒙集中在1~2岁之间，并且阅读时长是随着年龄的增长而增加的。\n",
    "\"\"\"\n",
    "\n",
    "run(prompt)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Runpod 部署 text-generation-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, runpod\n",
    "\n",
    "runpod.api_key = os.environ.get(\"RUNPOD_API_KEY\")\n",
    "\n",
    "num_shard = 1\n",
    "model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "quantize = \"bitsandbytes\"\n",
    "\n",
    "pod = runpod.create_pod(\n",
    "    name=\"Falcon-7B-Instruct-POD\",\n",
    "    image_name=\"ghcr.io/huggingface/text-generation-inference:latest\",\n",
    "    gpu_type_id=\"NVIDIA GeForce RTX 4080\",\n",
    "    cloud_type=\"COMMUNITY\",\n",
    "    docker_args=f\"--model-id {model_id} --num-shard {num_shard} --quantize {quantize}\",\n",
    "    gpu_count=num_shard,\n",
    "    volume_in_gb=50,\n",
    "    container_disk_in_gb=5,\n",
    "    ports=\"80/http\",\n",
    "    volume_mount_path=\"/data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deep learning is a branch of machine learning that uses artificial neural networks to learn from data and make predictions. It is based on the concept of hierarchical learning, where a model learns from a set of data and then applies that knowledge to new data. Deep learning has revolutionized the field of machine learning and has been used to solve complex problems such as image recognition, natural language processing, and self-driving cars.\n"
     ]
    }
   ],
   "source": [
    "from text_generation import Client\n",
    "\n",
    "client = Client(f\"https://{pod['id']}-80.proxy.runpod.net\")\n",
    "print(client.generate(\"What is Deep Learning?\", max_new_tokens=512).generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "\n",
    "inference_server_url_cloud = f\"https://{pod['id']}-80.proxy.runpod.net\"\n",
    "\n",
    "llm_cloud = HuggingFaceTextGenInference(\n",
    "    inference_server_url=inference_server_url_cloud,\n",
    "    max_new_tokens=1000,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.3,\n",
    "    repetition_penalty=1.03,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(template=\"{question}\", input_variables=[\"question\"])\n",
    "llm_chain_cloud = LLMChain(prompt=prompt, llm=llm_cloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".\\nI'm sorry, I don't understand what you're asking. Can you please provide more context or rephrase your question?\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain_cloud.run({\"question\" : \"your new question to falcon\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mactalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
