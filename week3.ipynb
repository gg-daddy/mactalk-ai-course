{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 思维链（CoT）的例子"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机三位数的乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"macktalk-ai-course\"\n",
    "print(os.environ.get(\"LANGCHAIN_PROJECT\"))\n",
    "print(os.environ.get(\"SERPAPI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import openai, os\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "COMPLETION_MODEL = \"text-davinci-003\"\n",
    "def get_response(prompt, model = COMPLETION_MODEL, temperature = 0.0, stop=None):\n",
    "    completions = openai.Completion.create(\n",
    "        engine=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=stop,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    message = completions['choices'][0]['text']  # type: ignore\n",
    "    return message\n",
    "\n",
    "# 生成两个三位数\n",
    "total = 100\n",
    "correct = 0\n",
    "\n",
    "def extract_numbers(text):\n",
    "    # 提取字符串中的所有数字\n",
    "    numbers = ''.join(char for char in text if char.isdigit())\n",
    "    # 将提取的数字转换为一个整数\n",
    "    return int(numbers) if numbers else None\n",
    "\n",
    "for i in range(total):\n",
    "    num1 = random.randint(100, 999)\n",
    "    num2 = random.randint(100, 999)\n",
    "\n",
    "    # 进行乘法运算\n",
    "    result = num1 * num2\n",
    "\n",
    "    question = f\"请计算{num1}乘以{num2}等于多少？答案是：\"\"\"\n",
    "    answer = get_response(question)\n",
    "    chatgpt_result = extract_numbers(answer)\n",
    "    if int(chatgpt_result) == int(result):\n",
    "        correct += 1\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"ChatGPT 结果 {chatgpt_result}\")\n",
    "    print(f\"计算器结果 {result}\")\n",
    "    # 输出结果\n",
    "    #print(f\"随机数1: {num1}, 随机数2: {num2}, 乘法结果: {result}\")\n",
    "\n",
    "print(f\"正确率: {correct / total}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 思维链的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai, os\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "COMPLETION_MODEL = \"text-davinci-002\"\n",
    "def get_response(prompt, model = COMPLETION_MODEL, temperature = 1.0, stop=None):\n",
    "    completions = openai.Completion.create(\n",
    "        engine=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=stop,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    message = completions['choices'][0]['text']  # type: ignore\n",
    "    return message\n",
    "\n",
    "question = \"\"\"A: 一个杂耍者有16个球。一半的球是高尔夫球，而高尔夫球的一半是蓝色的。那么有多少个蓝色高尔夫球呢？\n",
    "Q:\"\"\"\n",
    "\n",
    "print(get_response(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"\"\"A: 一个杂耍者有16个球。一半的球是高尔夫球，而高尔夫球的一半是蓝色的。那么有多少个蓝色高尔夫球呢？\n",
    "Q: 让我们一步一步想，\"\"\"\n",
    "\n",
    "print(get_response(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"利用以下线索回答下面的选择题。\n",
    "\n",
    "线索：\n",
    "1. Miss Scarlett是休息室中唯一的人。\n",
    "2. 抽烟斗的人在厨房里。\n",
    "3. Colonel Mustard是天文台中唯一的人。\n",
    "4. Professor Plum不在图书馆和台球室。\n",
    "5. 拿着蜡烛台的人在天文台。\n",
    "\n",
    "问题：Colonel Mustard是否在天文台中持着蜡烛台？\n",
    "(a) 是的；Colonel Mustard在天文台中持着蜡烛台。\n",
    "(b) 不是的；Colonel Mustard没有在天文台中持着蜡烛台。\n",
    "(c) 未知；没有足够的信息来确定Colonel Mustard是否在天文台中持着蜡烛台。\n",
    "\n",
    "解答：\"\"\"\n",
    "\n",
    "print(get_response(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"利用以下线索回答下面的选择题，使用以下步骤：\n",
    "(1) 首先，逐一查看线索并考虑该线索是否有潜在关联性。\n",
    "(2) 其次，结合相关线索来推理出问题的答案。\n",
    "(3) 最后，将答案映射到多项选择答案中的一个：要么是(a)、(b)或(c)。\n",
    "\n",
    "线索：\n",
    "1. Miss Scarlett是休息室中唯一的人。\n",
    "2. 抽烟斗的人在厨房里。\n",
    "3. Colonel Mustard是天文台中唯一的人。\n",
    "4. Professor Plum不在图书馆和台球室。\n",
    "5. 拿着蜡烛台的人在天文台。\n",
    "\n",
    "问题：Colonel Mustard是否在天文台中持着蜡烛台？\n",
    "(a) 是的；Colonel Mustard在天文台中持着蜡烛台。\n",
    "(b) 不是的；Colonel Mustard没有在天文台中持着蜡烛台。\n",
    "(c) 未知；没有足够的信息来确定Colonel Mustard是否在天文台中持着蜡烛台。\n",
    "\n",
    "解答：\n",
    "(1) 首先，逐一查看线索并考虑该线索是否有潜在关联性：\"\"\"\n",
    "\n",
    "print(get_response(question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least-to-most prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 注意，并不是每一次都会出错\n",
    "question = \"\"\"Q: It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to slide down. \n",
    "The water slide closes in 18 minutes. How many times can she slide before it closes? \"\"\"\n",
    "## Amy需要4分钟爬到滑梯的顶端，她需要1分钟滑下来。\n",
    "## 水滑梯将在15分钟后关闭。在水滑梯关闭之前，她可以滑多少次？\n",
    "print(get_response(question, model=\"text-davinci-003\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to slide down. \n",
    "The water slide closes in 18 minutes. \"\"\"\n",
    "\n",
    "step1 = \"\"\"\\nA: To solve \"How many times can she slide before it closes?\", we need to first solve: \\\"\"\"\"\n",
    "\n",
    "question1 = \"Q: \" + question + step1\n",
    "\n",
    "print(question1)\n",
    "answer1 = get_response(question1, model=\"text-davinci-003\", stop=\"\\n\")\n",
    "print(\"\\n\\n\")\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = question + \"\\nQ: \" + answer1 + \"?\\nA: \"\n",
    "print(question2)\n",
    "\n",
    "answer2 = get_response(question2, model=\"text-davinci-003\", stop=\"\\n\")\n",
    "print(answer2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question3 = question2 + answer2 + \"\\n Q: \" + \"How many times can she slide before it closes?\" + \"\\nA: \"\n",
    "print(question3)\n",
    "\n",
    "answer3 = get_response(question3, model=\"text-davinci-003\", stop=\"\\n\")\n",
    "print(answer3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用Least-to-most 思路来写单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\"# How to write great unit tests with pytest\n",
      "\n",
      "    In this advanced tutorial for experts, we'll use Python 3.10 and `pytest` to write a suite of unit tests to verify the behavior of the following function.\n",
      "    ```python\n",
      "    \n",
      "def format_time(seconds):\n",
      "    print(f\"Formatting {seconds} seconds\")\n",
      "    minutes, seconds = divmod(seconds, 60)\n",
      "    hours, minutes = divmod(minutes, 60)\n",
      "    if hours > 0:\n",
      "        return f\"{hours}h{minutes}min{seconds}s\"\n",
      "    elif minutes > 0:\n",
      "        return f\"{minutes}min{seconds}s\"\n",
      "    else:\n",
      "        return f\"{seconds}s\"\n",
      "\n",
      "    ```\n",
      "\n",
      "    Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n",
      "    - First,\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\"# How to write great unit tests with pytest\n",
      "\n",
      "    In this advanced tutorial for experts, we'll use Python 3.10 and `pytest` to write a suite of unit tests to verify the behavior of the following function.\n",
      "    ```python\n",
      "    \n",
      "def format_time(seconds):\n",
      "    print(f\"Formatting {seconds} seconds\")\n",
      "    minutes, seconds = divmod(seconds, 60)\n",
      "    hours, minutes = divmod(minutes, 60)\n",
      "    if hours > 0:\n",
      "        return f\"{hours}h{minutes}min{seconds}s\"\n",
      "    elif minutes > 0:\n",
      "        return f\"{minutes}min{seconds}s\"\n",
      "    else:\n",
      "        return f\"{seconds}s\"\n",
      "\n",
      "    ```\n",
      "\n",
      "    Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n",
      "    - First, the function takes in a single argument, `seconds`, which should be a number of seconds greater than or equal to 0.\n",
      "    - Next, the function prints a string to the console. The string includes the value of the `seconds` argument.\n",
      "    - The function then uses the `divmod()` built-in function to calculate the number of minutes and seconds. `divmod()` returns a tuple with the first element being the number of minutes and the second element being the number of seconds.\n",
      "    - The function then does the same for hours and minutes.\n",
      "    - Finally, the function returns a string formatted according to the number of hours, minutes, and seconds.\n",
      "        \n",
      "    A good unit test suite should aim to:\n",
      "    - Test the function's behavior for a wide range of possible inputs\n",
      "    - Test edge cases that the author may not have foreseen\n",
      "    - Take advantage of the features of `pytest` to make the tests easy to write and maintain\n",
      "    - Be easy to read and understand, with clean code and descriptive names\n",
      "    - Be deterministic, so that the tests always pass or fail in the same way\n",
      "\n",
      "    `pytest` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n",
      "\n",
      "    For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n",
      "    -\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\"# How to write great unit tests with pytest\n",
      "\n",
      "    In this advanced tutorial for experts, we'll use Python 3.10 and `pytest` to write a suite of unit tests to verify the behavior of the following function.\n",
      "    ```python\n",
      "    \n",
      "def format_time(seconds):\n",
      "    print(f\"Formatting {seconds} seconds\")\n",
      "    minutes, seconds = divmod(seconds, 60)\n",
      "    hours, minutes = divmod(minutes, 60)\n",
      "    if hours > 0:\n",
      "        return f\"{hours}h{minutes}min{seconds}s\"\n",
      "    elif minutes > 0:\n",
      "        return f\"{minutes}min{seconds}s\"\n",
      "    else:\n",
      "        return f\"{seconds}s\"\n",
      "\n",
      "    ```\n",
      "\n",
      "    Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n",
      "    - First, the function takes in a single argument, `seconds`, which should be a number of seconds greater than or equal to 0.\n",
      "    - Next, the function prints a string to the console. The string includes the value of the `seconds` argument.\n",
      "    - The function then uses the `divmod()` built-in function to calculate the number of minutes and seconds. `divmod()` returns a tuple with the first element being the number of minutes and the second element being the number of seconds.\n",
      "    - The function then does the same for hours and minutes.\n",
      "    - Finally, the function returns a string formatted according to the number of hours, minutes, and seconds.\n",
      "        \n",
      "    A good unit test suite should aim to:\n",
      "    - Test the function's behavior for a wide range of possible inputs\n",
      "    - Test edge cases that the author may not have foreseen\n",
      "    - Take advantage of the features of `pytest` to make the tests easy to write and maintain\n",
      "    - Be easy to read and understand, with clean code and descriptive names\n",
      "    - Be deterministic, so that the tests always pass or fail in the same way\n",
      "\n",
      "    `pytest` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n",
      "\n",
      "    For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n",
      "    - The `seconds` argument is a positive integer\n",
      "        - `seconds` is less than 60\n",
      "        - `seconds` is 60\n",
      "        - `seconds` is greater than 60\n",
      "    - The `seconds` argument is 0\n",
      "    - The `seconds` argument is a negative integer\n",
      "    - The `seconds` argument is not an integer\n",
      "        - `seconds` is a float\n",
      "        - `seconds` is a string\n",
      "        - `seconds` is `None`\n",
      "\n",
      "Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n",
      "```python\n",
      "import pytest  # used for our unit tests\n",
      "\n",
      "\n",
      "def format_time(seconds):\n",
      "    print(f\"Formatting {seconds} seconds\")\n",
      "    minutes, seconds = divmod(seconds, 60)\n",
      "    hours, minutes = divmod(minutes, 60)\n",
      "    if hours > 0:\n",
      "        return f\"{hours}h{minutes}min{seconds}s\"\n",
      "    elif minutes > 0:\n",
      "        return f\"{minutes}min{seconds}s\"\n",
      "    else:\n",
      "        return f\"{seconds}s\"\n",
      "\n",
      "\n",
      "#Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Generated unit test code:\n",
      "#Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\n",
      "@pytest.mark.parametrize(\n",
      "    \"seconds,expected\",\n",
      "    [\n",
      "        # Test cases for when the `seconds` argument is a positive integer\n",
      "        (0, \"0s\"),  # test when `seconds` is 0\n",
      "        (1, \"1s\"),  # test when `seconds` is less than 60\n",
      "        (59, \"59s\"),\n",
      "        (60, \"1min0s\"),  # test when `seconds` is 60\n",
      "        (61, \"1min1s\"),\n",
      "        (3599, \"59min59s\"),\n",
      "        (3600, \"1h0min0s\"),  # test when `seconds` is greater than 60\n",
      "        (3601, \"1h0min1s\"),\n",
      "        (3660, \"1h1min0s\"),\n",
      "        (7199, \"1h59min59s\"),\n",
      "        (7200, \"2h0min0s\"),\n",
      "        # Test case for when the `seconds` argument is a negative integer\n",
      "        (-1, \"Invalid input: seconds must be a positive integer\"),\n",
      "        # Test cases for when the `seconds` argument is not an integer\n",
      "        (1.5, \"Invalid input: seconds must be a positive integer\"),\n",
      "        (\"1\", \"Invalid input: seconds must be a positive integer\"),\n",
      "        (None, \"Invalid input: seconds must be a positive integer\"),\n",
      "    ],\n",
      ")\n",
      "def test_format_time(seconds, expected):\n",
      "    # The `seconds` argument is passed in as the first argument to the function\n",
      "    # The `expected` argument is passed in as the second argument to the function\n",
      "    assert format_time(seconds) == expected\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "\n",
    "def write_unit_test(function_to_test, unit_test_package = \"pytest\"):\n",
    "    # 解释源代码的步骤\n",
    "    explain_code = \"\"\"\"# How to write great unit tests with {unit_test_package}\n",
    "\n",
    "    In this advanced tutorial for experts, we'll use Python 3.10 and `{unit_test_package}` to write a suite of unit tests to verify the behavior of the following function.\n",
    "    ```python\n",
    "    {function_to_test}\n",
    "    ```\n",
    "\n",
    "    Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n",
    "    - First,\"\"\"\n",
    "\n",
    "    explain_code_template = PromptTemplate(\n",
    "        input_variables=[\"unit_test_package\", \"function_to_test\"],\n",
    "        template=explain_code\n",
    "    )\n",
    "    explain_code_llm = OpenAI(model_name=\"text-davinci-002\", temperature=0.4, max_tokens=1000,  # type: ignore\n",
    "            top_p=1, stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"])  # type: ignore\n",
    "    explain_code_step = LLMChain(llm=explain_code_llm, prompt=explain_code_template, output_key=\"code_explaination\", verbose=True)\n",
    "\n",
    "\n",
    "    # 创建测试计划示例的步骤\n",
    "    test_plan = \"\"\"\n",
    "        \n",
    "    A good unit test suite should aim to:\n",
    "    - Test the function's behavior for a wide range of possible inputs\n",
    "    - Test edge cases that the author may not have foreseen\n",
    "    - Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain\n",
    "    - Be easy to read and understand, with clean code and descriptive names\n",
    "    - Be deterministic, so that the tests always pass or fail in the same way\n",
    "\n",
    "    `{unit_test_package}` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n",
    "\n",
    "    For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n",
    "    -\"\"\"\n",
    "    test_plan_template = PromptTemplate(\n",
    "        input_variables=[\"unit_test_package\", \"function_to_test\", \"code_explaination\"],\n",
    "        template= explain_code + \"{code_explaination}\" + test_plan\n",
    "    )\n",
    "    test_plan_llm = OpenAI(model_name=\"text-davinci-002\", temperature=0.4, max_tokens=1000,   # type: ignore\n",
    "            top_p=1, stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"])  # type: ignore\n",
    "    test_plan_step = LLMChain(llm=test_plan_llm, prompt=test_plan_template, output_key=\"test_plan\",verbose=True)\n",
    "\n",
    "    # 撰写测试代码的步骤\n",
    "    starter_comment = \"Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\"\n",
    "    prompt_to_generate_the_unit_test = \"\"\"\n",
    "\n",
    "Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n",
    "```python\n",
    "import {unit_test_package}  # used for our unit tests\n",
    "\n",
    "{function_to_test}\n",
    "\n",
    "#{starter_comment}\"\"\"\n",
    "\n",
    "    unit_test_template = PromptTemplate(\n",
    "        input_variables=[\"unit_test_package\", \"function_to_test\", \"code_explaination\", \"test_plan\", \"starter_comment\"],\n",
    "        template= explain_code + \"{code_explaination}\" + test_plan + \"{test_plan}\" + prompt_to_generate_the_unit_test\n",
    "    )\n",
    "    unit_test_llm = OpenAI(model_name=\"text-davinci-002\", temperature=0.4, max_tokens=1000, stop=\"```\")  # type: ignore\n",
    "    unit_test_step = LLMChain(llm=unit_test_llm, prompt=unit_test_template, output_key=\"unit_test\",verbose=True)\n",
    "\n",
    "    sequential_chain = SequentialChain(chains=[explain_code_step, test_plan_step, unit_test_step],  # type: ignore\n",
    "                                    input_variables=[\"unit_test_package\", \"function_to_test\", \"starter_comment\"], verbose=True)\n",
    "    answer = sequential_chain.run(unit_test_package=unit_test_package, function_to_test=function_to_test, starter_comment=starter_comment)\n",
    "    return f\"\"\"#{starter_comment}\"\"\" + answer\n",
    "\n",
    "\n",
    "code = \"\"\"\n",
    "def format_time(seconds):\n",
    "    print(f\"Formatting {seconds} seconds\")\n",
    "    minutes, seconds = divmod(seconds, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    if hours > 0:\n",
    "        return f\"{hours}h{minutes}min{seconds}s\"\n",
    "    elif minutes > 0:\n",
    "        return f\"{minutes}min{seconds}s\"\n",
    "    else:\n",
    "        return f\"{seconds}s\"\n",
    "\"\"\"\n",
    "\n",
    "import ast\n",
    "\n",
    "def write_unit_test_automatically(code, retry=3):\n",
    "    unit_test_code = write_unit_test(code)\n",
    "    print(f\"Generated unit test code:\\n{unit_test_code}\")\n",
    "    all_code = code + unit_test_code\n",
    "    tried = 0\n",
    "    while tried < retry:\n",
    "        try:\n",
    "            ast.parse(all_code)\n",
    "            return all_code\n",
    "        except SyntaxError as e:\n",
    "            print(f\"Syntax error in generated code: {e}\")\n",
    "            all_code = code + write_unit_test(code)\n",
    "            tried += 1\n",
    "\n",
    "import pytest\n",
    "exec(write_unit_test_automatically(code))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-consistency Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Q: 如果停车场里有3辆汽车，又有2辆汽车到达，停车场里有多少辆汽车？\n",
    "A: 停车场里已经有3辆汽车。又有2辆汽车到达。现在总共有3 + 2 = 5辆汽车。答案是5辆。\n",
    "\n",
    "Q: Janet的鸭子每天下16个蛋。她每天早上吃掉其中三个，并用四个蛋每天烤松饼给她的朋友。她以每个蛋2美元的价格卖掉剩下的蛋。她每天能赚多少钱？\n",
    "A: \"\"\"\n",
    "\n",
    "\n",
    "COMPLETION_MODEL = \"text-davinci-002\"\n",
    "def get_top_response(prompt, n=10, model = COMPLETION_MODEL, temperature = 1.0, stop=None):\n",
    "    messages = []\n",
    "    completions = openai.Completion.create(\n",
    "        engine=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=1024,\n",
    "        n=n,\n",
    "        stop=stop,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    for choice in completions['choices']: # type: ignore\n",
    "        messages.append(choice['text'])\n",
    "    return messages\n",
    "\n",
    "answers = []\n",
    "\n",
    "answers = get_top_response(question, n=10, model=\"text-davinci-003\")\n",
    "for answer, i in zip(answers, range(10)):\n",
    "    print(\"Answer: \" + str(i))\n",
    "    print(answer)\n",
    "\n",
    "vote_question = \"\"\"\n",
    "Janet的鸭子每天下16个蛋。她每天早上吃掉其中三个，并用四个蛋每天烤松饼给她的朋友。她以每个蛋2美元的价格卖掉剩下的蛋。她每天能赚多少钱？\n",
    "\n",
    "请从以下5个人的答案中，选出票数最多答案作为最终答案。\n",
    "\"\"\"\n",
    "\n",
    "for answer, i in zip(answers, range(10)):\n",
    "    vote_question += f\"\\n答案{i}: {answer}\"\n",
    "\n",
    "final_answer = get_response(vote_question, model=\"text-davinci-003\")\n",
    "print(\"Final answer:\")\n",
    "print(final_answer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think step by step 计算三位数乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import openai, os\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "COMPLETION_MODEL = \"text-davinci-003\"\n",
    "def get_response(prompt, model = COMPLETION_MODEL, temperature = 0.0, stop=None):\n",
    "    completions = openai.Completion.create(\n",
    "        engine=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=stop,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    message = completions['choices'][0]['text']  # type: ignore\n",
    "    return message\n",
    "\n",
    "# 生成两个三位数\n",
    "total = 10\n",
    "correct = 0\n",
    "\n",
    "for i in range(total):\n",
    "    num1 = random.randint(100, 999)\n",
    "    num2 = random.randint(100, 999)\n",
    "\n",
    "    # 进行乘法运算\n",
    "    result = num1 * num2\n",
    "\n",
    "    question = f\"请计算{num1}乘以{num2}等于多少？请一步一步想。\"\"\"\n",
    "    answer = get_response(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"计算器结果 {result}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature参数的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax_with_temp(x, temp):\n",
    "    x = x / temp\n",
    "    e_x = np.exp(x - np.max(x))  # subtract max to avoid overflow\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "values = np.array([1.0, 2.0, 3.0])\n",
    "words = ['apple', 'banana', 'cherry']\n",
    "temps = [0.1, 1, 10]\n",
    "\n",
    "# setting up the figure with 3 subplots (in a vertical stack)\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 6))\n",
    "\n",
    "# setting up the positions for groups and bars within groups\n",
    "group_positions = np.arange(len(values))\n",
    "\n",
    "# creating the bars\n",
    "for ax, temp in zip(axes, temps):\n",
    "    probs = softmax_with_temp(values, temp)\n",
    "    ax.bar(group_positions, probs, label=f'Temp={temp}')\n",
    "    ax.set_xlabel('Words')\n",
    "    ax.set_ylabel('Probabilities')\n",
    "    ax.set_title(f'Softmax probabilities for Temp={temp}')\n",
    "    ax.set_xticks(group_positions)\n",
    "    ax.set_xticklabels(words)\n",
    "    ax.set_ylim([0, 1])  # setting y-axis limits\n",
    "    ax.legend()\n",
    "\n",
    "# adjusting the layout and showing the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRKL 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain, OpenAI, SerpAPIWrapper, SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "search = SerpAPIWrapper()\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
    "db = SQLDatabase.from_uri(\"sqlite:///./Chinook.db\")\n",
    "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events. You should ask targeted questions\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math_chain.run,\n",
    "        description=\"useful for when you need to answer questions about math\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"FooBar DB\",\n",
    "        func=db_chain.run,\n",
    "        description=\"useful for when you need to answer questions about FooBar. Input should be in the form of a question containing full context\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrkl = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who Leo DiCaprio's girlfriend is and then calculate her age raised to the 0.43 power.\n",
      "Action: Search\n",
      "Action Input: \"Who is Leo DiCaprio's girlfriend?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mWho is Leonardo DiCaprio currently dating? Leonardo DiCaprio has most recently been linked to 25-year-old Italian model Vittoria Ceretti. The couple were first spotted together in France in late May in the leadup to DiCaprio's appearance at the Cannes Film Festival.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate Vittoria Ceretti's age raised to the 0.43 power.\n",
      "Action: Calculator\n",
      "Action Input: 25^0.43\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "25^0.43\u001b[32;1m\u001b[1;3m```text\n",
      "25**0.43\n",
      "```\n",
      "...numexpr.evaluate(\"25**0.43\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m3.991298452658078\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 3.991298452658078\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Vittoria Ceretti's age raised to the 0.43 power is 3.991298452658078.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Vittoria Ceretti's age raised to the 0.43 power is 3.991298452658078.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrkl.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRKL面对开放复杂的问题容易失败"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to research the current progress of auto pilot for Tesla\n",
      "Action: Search\n",
      "Action Input: 'Current progress of auto pilot for Tesla'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mTesla's Autopilot is classified as Level 2 under the SAE six levels (0 to 5) of vehicle automation. At this level, the car can act autonomously, but requires the driver to monitor the driving at all times and be prepared to take control at a moment's notice.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find more information about the current progress of auto pilot for Tesla\n",
      "Action: Search\n",
      "Action Input: 'Recent developments in auto pilot for Tesla'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mDETROIT (AP) — Tesla is allowing some drivers to use its Autopilot driver-assist system for extended periods without making them put their hands on the steering wheel, a development that has drawn concern from U.S. safety regulators.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Tesla's Autopilot is classified as Level 2 under the SAE six levels (0 to 5) of vehicle automation. At this level, the car can act autonomously, but requires the driver to monitor the driving at all times and be prepared to take control at a moment's notice. Recent developments include Tesla allowing some drivers to use its Autopilot driver-assist system for extended periods without making them put their hands on the steering wheel, a development that has drawn concern from U.S. safety regulators.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Tesla's Autopilot is classified as Level 2 under the SAE six levels (0 to 5) of vehicle automation. At this level, the car can act autonomously, but requires the driver to monitor the driving at all times and be prepared to take control at a moment's notice. Recent developments include Tesla allowing some drivers to use its Autopilot driver-assist system for extended periods without making them put their hands on the steering wheel, a development that has drawn concern from U.S. safety regulators.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrkl.run([\"Write a 5000 words report about the current progress of auto pilot for Tesla.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to research the latest developments in Tesla's Autopilot technology\n",
      "Action: Search\n",
      "Action Input: 'Latest developments in Tesla Autopilot technology'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Autopilot is an advanced driver assistance system that enhances safety and convenience behind the wheel. When used properly, Autopilot reduces your overall ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find more information about the technology\n",
      "Action: Search\n",
      "Action Input: 'How does Tesla Autopilot work?'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThis advanced technology relies on eight cameras and robust neural net processing to perceive the car's surroundings and provide Autopilot features. By utilizing this camera suite, occupants are granted a heightened awareness of their environment, surpassing what a lone driver could perceive.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know enough to write the report\n",
      "Final Answer: A 5000-word technical report on the latest developments in Tesla Autopilot technology should include information on how the technology works, its features, and how it enhances safety and convenience for drivers.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A 5000-word technical report on the latest developments in Tesla Autopilot technology should include information on how the technology works, its features, and how it enhances safety and convenience for drivers.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrkl.run([\"写一篇5000字的关于当前特斯拉自动驾驶技术的最新进展的技术报告。\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGPT 写报告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.agents import Tool\n",
    "from langchain.tools.file_management.write import WriteFileTool\n",
    "from langchain.tools.file_management.read import ReadFileTool\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events. You should ask targeted questions\"\n",
    "    ),\n",
    "    WriteFileTool(),\n",
    "    ReadFileTool(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your embedding model\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "# Initialize the vectorstore as empty\n",
    "import faiss\n",
    "\n",
    "embedding_size = 1536\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.autonomous_agents import AutoGPT\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AutoGPT.from_llm_and_tools(\n",
    "    ai_name=\"Tom\",\n",
    "    ai_role=\"Assistant\",\n",
    "    tools=tools,\n",
    "    llm=ChatOpenAI(model_name=\"gpt-4\", temperature=0),\n",
    "    memory=vectorstore.as_retriever()\n",
    ")\n",
    "# Set verbose to be true\n",
    "agent.chain.verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are Tom, Assistant\n",
      "Your decisions must always be made independently without seeking user assistance.\n",
      "Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
      "If you have completed all your tasks, make sure to use the \"finish\" command.\n",
      "\n",
      "GOALS:\n",
      "\n",
      "1. Write a 5000 words report about the current progress of auto pilot for Tesla.\n",
      "\n",
      "\n",
      "Constraints:\n",
      "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
      "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
      "3. No user assistance\n",
      "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
      "\n",
      "Commands:\n",
      "1. search: useful for when you need to answer questions about current events. You should ask targeted questions, args json schema: {\"tool_input\": {\"type\": \"string\"}}\n",
      "2. write_file: Write file to disk, args json schema: {\"file_path\": {\"title\": \"File Path\", \"description\": \"name of file\", \"type\": \"string\"}, \"text\": {\"title\": \"Text\", \"description\": \"text to write to file\", \"type\": \"string\"}, \"append\": {\"title\": \"Append\", \"description\": \"Whether to append to an existing file.\", \"default\": false, \"type\": \"boolean\"}}\n",
      "3. read_file: Read file from disk, args json schema: {\"file_path\": {\"title\": \"File Path\", \"description\": \"name of file\", \"type\": \"string\"}}\n",
      "4. finish: use this to signal that you have finished all your objectives, args: \"response\": \"final response to let people know you have finished your objectives\"\n",
      "\n",
      "Resources:\n",
      "1. Internet access for searches and information gathering.\n",
      "2. Long Term memory management.\n",
      "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
      "4. File output.\n",
      "\n",
      "Performance Evaluation:\n",
      "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
      "2. Constructively self-criticize your big-picture behavior constantly.\n",
      "3. Reflect on past decisions and strategies to refine your approach.\n",
      "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
      "\n",
      "You should only respond in JSON format as described below \n",
      "Response Format: \n",
      "{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"thought\",\n",
      "        \"reasoning\": \"reasoning\",\n",
      "        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n",
      "        \"criticism\": \"constructive self-criticism\",\n",
      "        \"speak\": \"thoughts summary to say to user\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"command name\",\n",
      "        \"args\": {\n",
      "            \"arg name\": \"value\"\n",
      "        }\n",
      "    }\n",
      "} \n",
      "Ensure the response can be parsed by Python json.loads\n",
      "System: The current time and date is Fri Oct 13 23:32:40 2023\n",
      "System: This reminds you of these events from your past:\n",
      "[]\n",
      "\n",
      "\n",
      "Human: Determine which next command to use, and respond using the format specified above:\u001b[0m\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "The model `gpt-4` does not exist or you do not have access to it. Learn more: https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week3.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chenyanbin/codebase/ai/xuwenhao/mactalk-ai-course/week3.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m agent\u001b[39m.\u001b[39;49mrun([\u001b[39m\"\u001b[39;49m\u001b[39mWrite a 5000 words report about the current progress of auto pilot for Tesla.\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/langchain_experimental/autonomous_agents/autogpt/agent.py:93\u001b[0m, in \u001b[0;36mAutoGPT.run\u001b[0;34m(self, goals)\u001b[0m\n\u001b[1;32m     90\u001b[0m loop_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[39m# Send message to AI, get response\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m assistant_reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchain\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     94\u001b[0m     goals\u001b[39m=\u001b[39;49mgoals,\n\u001b[1;32m     95\u001b[0m     messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_history_memory\u001b[39m.\u001b[39;49mmessages,\n\u001b[1;32m     96\u001b[0m     memory\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory,\n\u001b[1;32m     97\u001b[0m     user_input\u001b[39m=\u001b[39;49muser_input,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Print Assistant thoughts\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mprint\u001b[39m(assistant_reply)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/langchain/chains/base.py:506\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    502\u001b[0m         _output_key\n\u001b[1;32m    503\u001b[0m     ]\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    507\u001b[0m         _output_key\n\u001b[1;32m    508\u001b[0m     ]\n\u001b[1;32m    510\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    511\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    512\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/langchain/chains/base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    305\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 306\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    307\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    308\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    309\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    310\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/langchain/chains/base.py:300\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    293\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    294\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    295\u001b[0m     inputs,\n\u001b[1;32m    296\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    297\u001b[0m )\n\u001b[1;32m    298\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 300\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    301\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    302\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    304\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    305\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/langchain/chains/llm.py:93\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     91\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 93\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/langchain/chains/llm.py:103\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    104\u001b[0m     prompts,\n\u001b[1;32m    105\u001b[0m     stop,\n\u001b[1;32m    106\u001b[0m     callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    107\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[1;32m    108\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/langchain/chat_models/base.py:463\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    456\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    457\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    461\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    462\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/langchain/chat_models/base.py:353\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    352\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 353\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    354\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    355\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    356\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    357\u001b[0m ]\n\u001b[1;32m    358\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/langchain/chat_models/base.py:343\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    341\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 343\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    344\u001b[0m                 m,\n\u001b[1;32m    345\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    346\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    347\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    348\u001b[0m             )\n\u001b[1;32m    349\u001b[0m         )\n\u001b[1;32m    350\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    351\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/langchain/chat_models/base.py:495\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    492\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    493\u001b[0m     )\n\u001b[1;32m    494\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 495\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    496\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    497\u001b[0m     )\n\u001b[1;32m    498\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/langchain/chat_models/openai.py:360\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    359\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m--> 360\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(\n\u001b[1;32m    361\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    362\u001b[0m )\n\u001b[1;32m    363\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/langchain/chat_models/openai.py:299\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/langchain/chat_models/openai.py:297\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/openai/api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    220\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    221\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    222\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/openai/api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    617\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    618\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[1;32m    620\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    621\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 624\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    625\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    626\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    627\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    628\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    629\u001b[0m         ),\n\u001b[1;32m    630\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    631\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mactalk/lib/python3.10/site-packages/openai/api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    686\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    688\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: The model `gpt-4` does not exist or you do not have access to it. Learn more: https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4."
     ]
    }
   ],
   "source": [
    "agent.run([\"Write a 5000 words report about the current progress of auto pilot for Tesla.\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "from termcolor import colored\n",
    "\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "USER_NAME = \"Person A\" # The name you want to use when interviewing the agent.\n",
    "LLM = ChatOpenAI(max_tokens=1500) # Can be any LLM you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.generative_agents import GenerativeAgent, GenerativeAgentMemory\n",
    "\n",
    "\n",
    "import math\n",
    "import faiss\n",
    "\n",
    "def relevance_score_fn(score: float) -> float:\n",
    "    \"\"\"Return a similarity score on a scale [0, 1].\"\"\"\n",
    "    # This will differ depending on a few things:\n",
    "    # - the distance / similarity metric used by the VectorStore\n",
    "    # - the scale of your embeddings (OpenAI's are unit norm. Many others are not!)\n",
    "    # This function converts the euclidean norm of normalized embeddings\n",
    "    # (0 is most similar, sqrt(2) most dissimilar)\n",
    "    # to a similarity function (0 to 1)\n",
    "    return 1.0 - score / math.sqrt(2)\n",
    "\n",
    "def create_new_memory_retriever():\n",
    "    \"\"\"Create a new vector store retriever unique to the agent.\"\"\"\n",
    "    # Define your embedding model\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "    # Initialize the vectorstore as empty\n",
    "    embedding_size = 1536\n",
    "    index = faiss.IndexFlatL2(embedding_size)\n",
    "    vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {}, relevance_score_fn=relevance_score_fn)\n",
    "    return TimeWeightedVectorStoreRetriever(vectorstore=vectorstore, other_score_keys=[\"importance\"], k=15)\n",
    "\n",
    "\n",
    "tommies_memory = GenerativeAgentMemory(\n",
    "    llm=LLM,\n",
    "    memory_retriever=create_new_memory_retriever(),\n",
    "    verbose=False,\n",
    "    reflection_threshold=8 # we will give this a relatively low number to show how reflection works\n",
    ")\n",
    "\n",
    "tommie = GenerativeAgent(name=\"Tommie\", \n",
    "              age=25,\n",
    "              traits=\"焦虑、喜欢设计、健谈\", # You can add more persistent traits here \n",
    "              status=\"正在找工作\", # When connected to a virtual world, we can have the characters update their status\n",
    "              memory_retriever=create_new_memory_retriever(),\n",
    "              llm=LLM,\n",
    "              memory=tommies_memory\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Tommie (age: 25)\n",
      "Innate traits: 焦虑、喜欢设计、健谈\n",
      "Tommie is straightforward, determined, and confident.\n"
     ]
    }
   ],
   "source": [
    "# The current \"Summary\" of a character can't be made because the agent hasn't made\n",
    "# any observations yet.\n",
    "print(tommie.get_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can add memories directly to the memory object\n",
    "tommie_observations = [\n",
    "    \"Tommie还记得小时候的狗Bruno。\",\n",
    "    \"Tommie因为长途驾车感到疲倦。\",\n",
    "    \"Tommie看到了新的住所。\",\n",
    "    \"新的邻居有一只猫。\",\n",
    "    \"夜晚道路很吵。\",\n",
    "    \"Tommie感到饥饿。\",\n",
    "    \"Tommie试图休息一下。\",\n",
    "]\n",
    "for observation in tommie_observations:\n",
    "    tommie.memory.add_memory(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Tommie (age: 25)\n",
      "Innate traits: 焦虑、喜欢设计、健谈\n",
      "Tommie is tired and in need of rest. They have memories of a dog named Bruno from their childhood. Tommie is also feeling hungry and has come across a new place to live. They notice that their new neighbor has a cat. Additionally, Tommie is bothered by the noisy road at night.\n"
     ]
    }
   ],
   "source": [
    "# Now that Tommie has 'memories', their self-summary is more descriptive, though still rudimentary.\n",
    "# We will see how this summary updates after more observations to create a more rich description.\n",
    "print(tommie.get_summary(force_refresh=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interview_agent(agent: GenerativeAgent, message: str) -> str:\n",
    "    \"\"\"Help the notebook user interact with the agent.\"\"\"\n",
    "    new_message = f\"{USER_NAME} says {message}\"\n",
    "    return agent.generate_dialogue_response(new_message)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tommie said \"我喜欢设计和艺术。你呢？\"'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interview_agent(tommie, \"你喜欢干什么?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tommie said \"我今天打算休息一下。昨晚的夜路太吵了，我感到有些疲倦。你呢，今天有什么计划吗？\"'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interview_agent(tommie, \"你今天打算干些什么?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tommie said \"我今天最担心的事情是找到一份工作。我正在积极寻找工作机会，但目前还没有成功。希望能尽快找到一份满意的工作。你呢？你今天最担心的是什么？\"'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interview_agent(tommie, \"你今天最担心的事情是什么?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mactalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
